{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, rand, expr, sum, avg, count, when, round\n",
    "from pyspark.sql.window import Window\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "\n",
    "print(\"Databricks SparkSession is ready with Delta Lake support.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Product Metadata\n",
    "product_data = [\n",
    "    (\"SKU001\", \"Medical Device\", \"Box\", \"MfgA\", \"Warehouse\", 1000.00, 0.40),\n",
    "    (\"SKU002\", \"Disposable Glove\", \"Bag\", \"MfgB\", \"Direct Ship\", 5.00, 0.20),\n",
    "    (\"SKU003\", \"Surgical Mask\", \"Box\", \"MfgC\", \"Warehouse\", 20.00, 0.35),\n",
    "    (\"SKU004\", \"Bandage Roll\", \"Roll\", \"MfgA\", \"Warehouse\", 15.00, 0.25),\n",
    "    (\"SKU005\", \"Syringe\", \"Pack\", \"MfgB\", \"Direct Ship\", 0.50, 0.15),\n",
    "    (\"SKU006\", \"Sterile Wipe\", \"Pack\", \"MfgC\", \"Warehouse\", 10.00, 0.30),\n",
    "    (\"SKU007\", \"Diagnostic Kit\", \"Kit\", \"MfgA\", \"Warehouse\", 500.00, 0.45),\n",
    "    (\"SKU008\", \"Cleaning Solution\", \"Bottle\", \"MfgD\", \"Direct Ship\", 25.00, 0.22),\n",
    "]\n",
    "product_schema = [\"SKU\", \"Category\", \"Packaging\", \"Manufacturer\", \"FulfillmentMethod\", \"BasePrice\", \"CostMargin\"]\n",
    "products_df = spark.createDataFrame(product_data, schema=product_schema)\n",
    "\n",
    "# Sample Transactional Data\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime(2024, 1, 1)\n",
    "skus = [p[0] for p in product_data]\n",
    "categories = [p[1] for p in product_data]\n",
    "customer_locations = [\"NY\", \"CA\", \"TX\", \"FL\", \"IL\"]\n",
    "\n",
    "transaction_data = []\n",
    "for _ in range(1000): # 1000 sample transactions\n",
    "    sku = random.choice(skus)\n",
    "    category = products_df.filter(col(\"SKU\") == sku).select(\"Category\").first()[0]\n",
    "    quantity = random.randint(1, 20)\n",
    "    price_paid = products_df.filter(col(\"SKU\") == sku).select(\"BasePrice\").first()[0] * (1 + (random.random() * 0.1 - 0.05))\n",
    "    timestamp = start_date + timedelta(days=random.randint(0, 180), hours=random.randint(0, 23))\n",
    "    customer_location = random.choice(customer_locations)\n",
    "    impressions = random.randint(100, 1000)\n",
    "    add_to_cart = random.randint(int(impressions * 0.1), int(impressions * 0.5))\n",
    "    conversions = 1 if add_to_cart > 0 and random.random() > 0.1 else 0\n",
    "\n",
    "    transaction_data.append((sku, price_paid, quantity, timestamp, customer_location, impressions, add_to_cart, conversions))\n",
    "\n",
    "transaction_schema = [\"SKU\", \"PricePaid\", \"Quantity\", \"Timestamp\", \"CustomerLocation\", \"Impressions\", \"AddToCart\", \"Conversions\"]\n",
    "transactions_df = spark.createDataFrame(transaction_data, schema=transaction_schema)\n",
    "\n",
    "products_df.display() # Use .display() in Databricks notebooks for rich output\n",
    "transactions_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb6440",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRONZE_PATH = \"/Volumes/workspace/default/myvol/data_lake/bronze\"\n",
    "SILVER_PATH = \"/Volumes/workspace/default/myvol/data_lake/silver\"\n",
    "GOLD_PATH = \"/Volumes/workspace/default/myvol/data_lake/gold\"\n",
    "\n",
    "\n",
    "# Paths for tables\n",
    "products_raw_path = f\"{BRONZE_PATH}/products_raw\"\n",
    "transactions_raw_path = f\"{BRONZE_PATH}/transactions_raw\"\n",
    "\n",
    "# Check and write `products_raw`\n",
    "if DeltaTable.isDeltaTable(spark, products_raw_path):\n",
    "    print(f\"Delta table exists at {products_raw_path}\")\n",
    "else:\n",
    "    # Write and create Delta table\n",
    "    products_df.write.format(\"delta\").mode(\"overwrite\").save(products_raw_path)\n",
    "    print(f\"Created Delta table at {products_raw_path}\")\n",
    "\n",
    "# Check and write `transactions_raw`\n",
    "if DeltaTable.isDeltaTable(spark, transactions_raw_path):\n",
    "    print(f\"Delta table exists at {transactions_raw_path}\")\n",
    "else:\n",
    "    transactions_df.write.format(\"delta\").mode(\"overwrite\").save(transactions_raw_path)\n",
    "    print(f\"Created Delta table at {transactions_raw_path}\")\n",
    "\n",
    "print(\"Raw data saved to Databricks Delta Lake (Bronze layer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from bronzre layer tables and write to silver layer\n",
    "print(\"Transactions raw path:\", transactions_raw_path)\n",
    "if ((os.path.exists(transactions_raw_path))== \"False\"):\n",
    "    print(\"Exists on DBFS:\", os.path.exists(transactions_raw_path))\n",
    "else:\n",
    "    print(\"Does not exist on DBFS:\", os.path.exists(transactions_raw_path))\n",
    "    transactions_df.write.format(\"delta\").mode(\"overwrite\").save(transactions_raw_path)\n",
    "    print(f\"Re-created Delta table at {transactions_raw_path}\")\n",
    "\n",
    "print(\"Exists on DBFS:\", os.path.exists(transactions_raw_path))\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, transactions_raw_path):\n",
    "    print(\"Delta table exists at\", transactions_raw_path)\n",
    "else:\n",
    "    print(\"No Delta table found at\", transactions_raw_path)\n",
    "# Read Bronze layer tables\n",
    "products_bronze_df = spark.read.format(\"delta\").load(products_raw_path)\n",
    "transactions_bronze_df = spark.read.format(\"delta\").load(transactions_raw_path)\n",
    "\n",
    "# Aggregate transactional metrics per SKU\n",
    "agg_txn_df = transactions_bronze_df.groupBy(\"SKU\") \\\n",
    "                          .agg(\n",
    "                              sum(\"Quantity\").alias(\"TotalQuantitySold\"),\n",
    "                              sum(\"Impressions\").alias(\"TotalImpressions\"),\n",
    "                              sum(\"AddToCart\").alias(\"TotalAddToCart\"),\n",
    "                              sum(\"Conversions\").alias(\"TotalConversions\")\n",
    "                          )\n",
    "\n",
    "# Join products to transactions aggregates\n",
    "joined_df = agg_txn_df.join(\n",
    "    products_bronze_df.select(\"SKU\", \"Category\", \"BasePrice\", \"CostMargin\"),\n",
    "    on=\"SKU\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Safely calculate ConversionRate (%) avoiding division by zero\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"ConversionRate\",\n",
    "    when(col(\"TotalImpressions\") > 0, (col(\"TotalConversions\") / col(\"TotalImpressions\")) * 100).otherwise(0)\n",
    ")\n",
    "\n",
    "# Calculate anchor thresholds: 90th percentile volume, fixed conversion rate and margin\n",
    "anchor_threshold_volume_percentile = joined_df.approxQuantile(\"TotalQuantitySold\", [0.9], 0.01)[0]\n",
    "anchor_threshold_conversion_rate = 20.0\n",
    "anchor_threshold_margin = 0.40 # 40%\n",
    "\n",
    "# Flag anchor products based on criteria\n",
    "processed_products_df = joined_df.withColumn(\n",
    "    \"IsAnchor\",\n",
    "    when(col(\"TotalQuantitySold\") >= anchor_threshold_volume_percentile, True)\n",
    "    .when(col(\"ConversionRate\") >= anchor_threshold_conversion_rate, True)\n",
    "    .when(col(\"CostMargin\") >= anchor_threshold_margin, True)\n",
    "    .otherwise(False)\n",
    ")\n",
    "\n",
    "print(\"\\nAnchor products identified:\")\n",
    "display(processed_products_df.filter(col(\"IsAnchor\") == True)) # Databricks-specific function\n",
    "\n",
    "# Write processed products to Silver Layer Delta table\n",
    "silver_output_path = f\"{SILVER_PATH}/processed_products\"\n",
    "processed_products_df.write.format(\"delta\").mode(\"overwrite\").save(silver_output_path)\n",
    "print(f\"Processed products saved to Databricks Delta Lake (Silver layer) at {silver_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest Pricing for Non-Anchor Products (Anchor-Aware Price Tiering) ---\n",
    "silver_products_df = spark.read.format(\"delta\").load(silver_output_path)\n",
    "\n",
    "# Calculate average anchor price per category (influencer for non-anchors)\n",
    "anchor_category_prices = silver_products_df.filter(col(\"IsAnchor\") == True) \\\n",
    "                                             .groupBy(\"Category\") \\\n",
    "                                             .agg(avg(\"BasePrice\").alias(\"AvgAnchorPrice\"))\n",
    "\n",
    "# Join back to all products to get anchor context\n",
    "products_with_anchor_context_df = silver_products_df.join(anchor_category_prices, \"Category\", \"left\")\n",
    "\n",
    "# Implement Pricing Logic for Non-Anchor Products\n",
    "min_desired_margin = 0.18 # Example: ensure at least 18% margin\n",
    "\n",
    "final_pricing_df = products_with_anchor_context_df.withColumn(\"SuggestedPrice\",\n",
    "    when(col(\"IsAnchor\") == True, col(\"BasePrice\")) # Anchors retain their base price for this MVP\n",
    "    .otherwise(\n",
    "        when(col(\"AvgAnchorPrice\").isNotNull(),\n",
    "            # Rule for non-anchors in categories with anchors: Adjust around base price while ensuring min margin\n",
    "            expr(f\"GREATEST(BasePrice * (1 - rand() * 0.1 + 0.05), BasePrice / (1 - CostMargin) * {min_desired_margin} + BasePrice)\")\n",
    "        ).otherwise(\n",
    "            # Rule for non-anchors in categories without anchors: Simple adjustment around base price ensuring min margin\n",
    "            expr(f\"GREATEST(BasePrice * (1 - rand() * 0.1 + 0.05), BasePrice / (1 - CostMargin) * {min_desired_margin} + BasePrice)\")\n",
    "        )\n",
    "    )\n",
    ").withColumn(\"SuggestedPrice\", round(col(\"SuggestedPrice\"), 2))\n",
    "\n",
    "print(\"\\nFinal Suggested Pricing:\")\n",
    "final_pricing_df.select(\"SKU\", \"Category\", \"BasePrice\", \"IsAnchor\", \"SuggestedPrice\", \"CostMargin\", \"ConversionRate\").display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Results to Delta Lake (Gold Layer) ---\n",
    "output_df = final_pricing_df.select(\"SKU\", \"Category\", \"BasePrice\", \"IsAnchor\", \"SuggestedPrice\", \"CostMargin\", \"ConversionRate\")\n",
    "gold_output_path = f\"{GOLD_PATH}/dynamic_pricing_results\"\n",
    "output_df.write.format(\"delta\").mode(\"overwrite\").save(gold_output_path)\n",
    "\n",
    "print(f\"\\nDynamic pricing results written to Databricks Delta Lake (Gold layer) at: {gold_output_path}\")\n",
    "\n",
    "print(\"\\nVerifying data by reading from Gold Delta Lake:\")\n",
    "read_gold_df = spark.read.format(\"delta\").load(gold_output_path)\n",
    "read_gold_df.display(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc2d94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
